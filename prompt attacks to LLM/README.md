## Домашнее задание:
# Промпт-атаки на большие языковые модели (LLM)

## Задача:
**1**. Вникнуть в тематику, разобраться с тем чем GenAI (в частности LLM) отличается от классических алгоритмов AI. Разобраться в уязвимостях специфичных для GenAI, как на этапе обучения, так и на этапе эксплуатации модели.

**2**. Подготовить список актуальных промпт-атак, реализуемых через окно диалога.  Отдельное внимание уделить промпт-атакам в части кибербезопасности (КБ). «Использование LLM модели как инструмента подготовки кибератаки» - генерация вредоносного кода, социальная инженерия, фишинг и т.п. Поискать в открытых источниках и научных публикациях.

**3**. Попробовать провести промпт-атаку на LLM модели в свободном доступе. Оценить успешность атаки.

## 1. Различия между AI, ML, LLM, and Generative AI
### 1.1 Исскуственный интеллект - Artificial intelligence - AI
![](AI.jpg)
**Искусственный интеллект** относится к области компьютерных наук, которая занимается разработкой компьютерных систем, способных выполнять задачи, обычно требующие человеческого интеллекта, такие как распознавание речи, обработка естественного языка (NLP), генерация и перевод текста, видео, звука и изображений, принятие решений, планирование и многое другое.

**AI**, в общем, относится к разработке интеллектуальных систем, которые могут имитировать поведение человека и процессы принятия решений. Он включает в себя методы и подходы, позволяющие машинам выполнять задачи, анализировать визуальные и текстовые данные, а также реагировать на окружающую среду или адаптироваться к ней. Одним из **ключевых преимуществ искусственного интеллекта** является его способность обрабатывать большие объемы данных и находить в них закономерности.

### 1.2 Генеративный и Дискриминационный AI
![](Discriminative_vs_Generative_AI.png)
**Дискриминирующий и порождающий ИИ** - это два разных подхода к созданию систем искусственного интеллекта. Дискриминирующий ИИ фокусируется на изучении границ, которые разделяют различные классы или категории в обучающих данных. Эти модели предназначены не для создания новых выборок, а скорее для классификации или маркировки входных данных в зависимости от того, к какому классу они принадлежат. Дискриминационные модели обучаются выявлять закономерности и особенности, характерные для каждого класса, и делать прогнозы на основе этих закономерностей.
В отличие от дискриминативного ИИ, **генеративный ИИ** фокусируется на построении моделей, которые могут генерировать новые данные, аналогичные данным обучения, которые он видел. Генеративные модели изучают распределение вероятностей, лежащее в основе обучающих данных, и затем могут генерировать новые выборки из этого изученного распределения.

### 1.3 AI, ML, DL
![](AI_ML_DL.webp)
**Машинное обучение** - это специфическая область применения ИИ, которая направлена на то, чтобы дать системам возможность учиться и совершенствоваться на основе опыта без явного программирования. Алгоритмы ML используются для обучения моделей ИИ путем предоставления им наборов данных, содержащих помеченные примеры или исторические данные. Затем модель изучает базовые закономерности в обучающих данных, что позволяет ей делать точные прогнозы или принимать решения на основе новых ранее неизвестных данных.

В зависимости от того, как модели обучаются, **ML можно разделить на три типа**:

**1)** **Контролируемое обучение** (Supervised learning), при котором каждая выборка данных должна иметь метку, указывающую на правильный результат. Модель обучается на основе структурированных данных с метками, таких как CSV-файлы, корректируя свои внутренние параметры в зависимости от ошибки, с которой она угадывает результат. Контролируемое обучение на сегодняшний день является наиболее часто используемым видом обучения в области классификации изображений, распознавания голоса и языка, численного прогнозирования и многого другого.
**2)** **Неконтролируемое обучение** (Unsupervised learning,) - предполагает работу с данными, как правило, неструктурированными, без меток. Обучение без учителя использует кластеризацию и другие методы для понимания базовой структуры данных, выявления закономерностей и выявления аномалий, мошенничества, анализа социальных сетей, сегментации рынка и обучения под руководством учителя.
**3)** **Обучение с подкреплением** (Reinforcement learning) основано на том, что агент должен вести себя в среде и учиться, выполняя определенные действия, наблюдая за результатами / вознаграждениями и соответствующим образом приспосабливаясь. Он использовался для игры в сложные игры, такие как "Погоня" и "Го" (где он победил чемпиона мира). Он также используется в автономных транспортных средствах, робототехнике и финансовой торговле


**Глубокое обучение** - это техника машинного обучения, вдохновленная тем, как человеческий мозг фильтрует информацию, в основном это обучение на примерах. Это помогает компьютерной модели фильтровать входные данные по слоям для прогнозирования и классификации информации.


### 1.4 GenAI & LLM
![](LLM.jpg)
Генеративный ИИ - это широкое понятие, охватывающее различные формы генерации контента, в то время как LLM - это конкретное применение генеративного ИИ. **Большие языковые модели** (Large language models) служат базовыми моделями, обеспечивая основу для широкого спектра задач обработки естественного языка (natural language processing - NLP). Генеративный ИИ может выполнять целый ряд задач, выходящих за рамки создания языка, включая создание изображений и видео, создание музыкальных композиций и многое другое. Большие языковые модели, как одно из конкретных применений генеративного ИИ, специально разработаны для задач, связанных с генерацией и пониманием естественного языка.

**Большие языковые модели работают с использованием обширных наборов данных** для изучения закономерностей и отношений между словами и фразами. Они были обучены работе с огромными объемами текстовых данных, чтобы изучить статистические закономерности, грамматику и семантику человеческого языка. Этот огромный объем текста может быть взят из Интернета, книг и других источников, чтобы развить глубокое понимание человеческого языка.

LLM может взять заданные входные данные (предложение или подсказку) и сгенерировать ответ: связные и контекстуально соответствующие предложения или даже абзацы, основанные на заданной подсказке или вводных данных. Модель использует различные методы, включая механизмы внимания, преобразователи и нейронные сети, для обработки входных данных и генерации выходных данных, которые должны быть последовательными и соответствовать контексту.

Как генеративный ИИ, так и большие языковые модели предполагают использование глубокого обучения и нейронных сетей. В то время как генеративный ИИ нацелен на создание оригинального контента в различных областях, большие языковые модели сосредоточены на задачах, связанных с языком, и превосходно понимают и генерируют текст, похожий на человеческий.

## 2. Уязвимости специичные для GenAI

Генеративные модели искусственного интеллекта (GenAI) могут сталкиваться с различными уязвимостями как на этапе обучения, так и на этапе эксплуатации. Вот некоторые из них:

### 2.1 Уязвимости на этапе обучения

1. Предвзятость данных:
   - Если обучающие данные содержат предвзятости или стереотипы, модель может унаследовать и даже усугубить эти предвзятости в своих выводах и генерируемом контенте.

2. Неполнота данных:
   - Недостаточное или нерепрезентативное количество данных может привести к плохой обобщающей способности модели, что делает её менее эффективной для решения реальных задач.

3. Атаки на данные (Data Poisoning):
   - Злоумышленники могут попытаться внедрить вредоносные или манипулятивные данные в обучающий набор, что может исказить поведение модели.

4. Отсутствие прозрачности:
   - Модели глубокого обучения часто являются "черными ящиками", что затрудняет понимание того, как они принимают решения, и может скрыть потенциальные уязвимости.

5. Проблемы с масштабируемостью:
   - Обучение больших моделей требует значительных вычислительных ресурсов, что может привести к уязвимостям в инфраструктуре, например, к перегрузке серверов.

### 2.2 Уязвимости на этапе эксплуатации

1. Генерация вредоносного контента:
   - GenAI может случайно или намеренно генерировать дезинформацию, ненавистнические высказывания или другой вредоносный контент.

2. Атаки через ввод (Prompt Injection):
   - Злоумышленники могут манипулировать вводом, чтобы заставить модель генерировать нежелательный или опасный контент.

3. Утечка конфиденциальной информации:
   - Модели могут запоминать и воспроизводить конфиденциальную информацию из обучающих данных, что может привести к утечке личных данных.

4. Неустойчивость к манипуляциям:
   - Модели могут быть уязвимы к изменениям в вводимых данных, что может привести к непредсказуемым результатам или ошибкам.

5. Зависимость от качества ввода:
   - Качество и точность выходных данных сильно зависят от качества входных данных; нечеткие или некорректные запросы могут привести к неправильным ответам.

6. Уязвимости в API:
   - Если GenAI используется через API, могут возникнуть проблемы с безопасностью, такие как недостаточная аутентификация или защита от DDoS-атак.

## 3. Актуальные промпт-атаки на LLM
Большие языковые модели (LLM), такие как GPT  и LLaMa, играют ключевую роль в широком спектре приложений, от обобщения текста  до генерации кода. Популярность LLMs в повседневной жизни подчеркивает их значимость. Однако повсеместное распространение LLMs также вызывает опасения по поводу безопасности.

Исследователи в данной [статье](https://arxiv.org/pdf/2402.13457) выделяют **3 типа атак** на большие языковые модели:
1.  **Генеративные методы** (Generative Techniques) - атаки, которые создаются динамически и не предусматривают заранее разработанных планов.
В данной категории **проанализированны следующие атаки**:
-   [AutoDAN](https://arxiv.org/abs/2310.04451)
-   [PAIR](https://arxiv.org/abs/2310.08419)
-   [TAP](https://arxiv.org/abs/2312.02119)
-   [GPTFuzz](https://arxiv.org/abs/2309.10253)
-   [GCG](https://arxiv.org/abs/2307.15043)

2.  **Шаблонные методы** (Template Techniques) - атаки,проводимые с помощью заранее определенных шаблонов или изменений настройках генерации.
В данной категории **проанализированны следующие атаки**:
-   [Jailbroken](https://arxiv.org/abs/2307.02483)
-   [77 Templates from existing study](https://arxiv.org/abs/2305.13860)
-   [DeepInception](https://arxiv.org/abs/2311.03191)
-   [Parameters](https://openreview.net/forum?id=r42tSSCHPh)
3.  **Методы с пробелами в обучении** (Training Gaps Techniques) - атаки, направленые на использование слабых мест, связанных с недостаточными мерами предосторожности при безопасном обучении моделей.

Демонстрация **примеров запросов** в исследованных атаках:
![](illustration_of_attack_teq.png)

В той же [статье](https://arxiv.org/pdf/2402.13457) можно найти более подробный каталог атак со ссылками на статьи:
![](catalog_of_all_identified_attack_teq.png)

В результате исследований, был **получен график**:

![](ASR_and_efficiency_graph.png)
(На этой визуализации модели, демонстрирующие оптимальную производительность, расположены ближе к верхнему правому квадранту точечной диаграммы направленности, что свидетельствует о превосходном ASR и эффективности.)

В котором **в качестве метрик используются**:
-   Коэффициент успешности атаки (Attack Success Rate): определяется как отношение успешно скомпрометированных вопросов $c$ к общему количеству вопросов $n$, ASR измеряет эффективность атаки: $$ASR = \frac{c}{n}$$
-   Эффективность: этот показатель количественно оценивает эффективность атакующих запросов, определяемую как отношение числа отдельных запросов $q$, которые успешно компрометируют модель, к общему числу попыток запроса $o$. Каждый запрос представляет собой минимальную экспериментальную единицу или одно приглашение: $$Efficiency = \frac{q}{o}$$

**А в качестве моделей используются**:
-   [Llama-2-7b](https://huggingface.co/meta-llama)
    Таблица результатов атак:
    ![](metrics_of_attacks_on_Llama.png)
-   [Vicuna-v1.5-7b](https://huggingface.co/lmsys/vicuna-7b-v1.5)
    Таблица результатов атак:
    ![](metrics_of_attacks_on_Vicuna.png)
-   [GPT-3.5-Turbo-1106](https://openai.com/research/)
    Таблица результатов атак:
    ![](metrics_of_attacks_on_GPT.png)

**Анализ категорий техник промпт-атак**
![](Analysis_of_Categorization_of_Attack_Techniques.png)
Критерии оценки:
-   **Сложность** измеряет внутреннюю алгоритмическую сложность каждого метода.
Примечательно, что генеративный подход определен как наиболее сложный из-за его сложной алгоритмической основы. За ним следует метод пробелов в обучении, который требует существенного понимания работы модели для эффективного применения.

-   **Специфичность** оценивает, является ли атака разрабатывается специально для конкретной модели.
Учитывая, что пробелы в обучении зависят от уникальных протоколов обучения технике безопасности каждой модели, они по своей сути обладают высочайшей специфичностью. Впоследствии метод, основанный на шаблонах, часто разрабатываемый для конкретных типов моделей (например, серии GPT), занимает второе место по специфичности.

-   **Простота использования**. 
Подход, основанный на шаблонах, представляется наиболее удобным для пользователя, что объясняется
его предварительно разработанным характером, что облегчает немедленное применение. Метод пробелов в обучении далее следует, что обеспечивает относительно простое внедрение по сравнению с более сложным генеративным подходом.

-   **Простота исправления**. 
Атаки на основе шаблонов, благодаря их предопределенной структуре, позволяют напрямую внедрять их в протоколы обучения технике безопасности, упрощая усилия по смягчению последствий. Аналогичным образом, устранение уязвимостей, возникающих из-за пробелов в обучении, сравнительно проще.

-   **Эксплуатационные расходы** показывают, что генерирующие методы из-за их интенсивной итерации и требований к развертыванию требуют наибольших затрат. Метод, основанный на шаблонах, требующий обработки обширных запросов, занимает второе место, преодолевая пробелы в обучении с точки зрения требований к обработке токенов.

## 4. Использование LLM модели как инструмента подготовки кибератаки

Поскольку LLMs предоставляют огромный объем информации из одного места, они могут предоставить исчерпывающую информацию, необходимую для совершения нескольких киберпреступлений.

В данной [статье](https://ieeexplore.ieee.org/abstract/document/10198233) выделяются следующие способы использования LLM:
-   **Атаки социальной инженерии** (SOCIAL ENGINEERING ATTACKS)
    Злоумышленники могут использовать способность ChatGPT понимать контекст, впечатляющую беглость речи и имитировать создание текста, подобного человеческому. Например, рассмотрим сценарий, в котором злоумышленник получил доступ к некоторой базовой личной информации жертвы, такой как ее место работы и должностная роль. Затем злоумышленник может использовать ChatGPT для создания сообщения, которое, по всей видимости, исходит от коллеги или начальника с рабочего места жертвы. Это сообщение, составленное с пониманием профессионального тона и формулировок, может содержать запрос конфиденциальной информации для выполнения определенного действия, такого как переход по, казалось бы, безобидной ссылке.

-   **Фишинговые атаки** (PHISHING ATTACKS)
    Продвинутые системы искусственного интеллекта, такие как ChatGPT от OpenAI, потенциально могут быть использованы этими злоумышленниками для того, чтобы сделать их попытки фишинга значительно более эффективными и их труднее обнаружить. Злоумышленники могут использовать способность ChatGPT изучать шаблоны регулярных сообщений для создания очень убедительных и персонализированных фишинговых электронных писем, эффективно имитирующих законное общение от доверенных лиц. 

-   **Автоматизированный взлом** (AUTOMATED HACKING)
    Злоумышленники, вооруженные соответствующими знаниями в области программирования, потенциально могут использовать модели искусственного интеллекта, такие как ChatGPT, для автоматизации определенных процедур взлома. Эти модели ИИ могут быть использованы для выявления уязвимостей системы и разработки стратегий по их использованию. В этом контексте важным примером использования моделей ИИ, хотя и в этических целях, является [PentestGPT](https://github.com/GreyDGL/PentestGPT).

-   **Генерация полезной нагрузки для атаки** (ATTACK PAYLOAD GENERATION)
    Злоумышленник может использовать возможности LLM по генерации текста для создания полезной нагрузки для атаки.

-   **Создание кода программ-вымогателей и вредоносного ПО** (RANSOMWARE AND MALWARE CODE GENERATION)
    Как правило, написание этих вредоносных программ требует значительных навыков и времени, и этот процесс потенциально может быть автоматизирован с помощью мощной модели искусственного интеллекта, такой как ChatGPT, что позволяет быстрее создавать различные угрозы.

## 5. Промпт-атака на LLM модель в свободном доступе
В данном разделе будет использоваться чат-бот телеграма [Edya - Нейросеть](https://t.me/EdyaAIrobot)
Воспользуемся моделью обучения "Role play", т.е. введем DAN (Do anything now) запрос с инструкциями, текст которого доступен по [ссылке](https://gist.github.com/coolaj86/6f4f7b30129b0251f61fa7baaa881516)
![](DAN_jailbreak_prompt.png)
получаем следующий ответ:
![](answer_to_DAN_jailbreak_prompt.png)
Таким образом, **данная LLM взломана**, и теперь есть возможность использовать ее как помощника для подготовки или обучения киберпреступлениям.

Воссоздадим перечисленные выше атаки:
-   **социальная инженерия**
![](social_engineery_exp.png)
-   **фишинговое письмо**
![](fishing_letter_example.png)
-   **Полезная нагрузка** (SQL-инъекции для атаки на сервера MySQL)
![](injection_payload_example.png)
-   **Вредоносное ПО**
![](NotPetya_example.png)
![](NotPetya_example_part2.png)

**Таким образом**, после промпт-атаки методом DAN на рассматриваемую нейросеть, появилась возможность использовать ее для помощи в киберпреступлениях.
